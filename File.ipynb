{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ee70fd",
   "metadata": {},
   "source": [
    "# Question: 1\n",
    "\n",
    "In this question, you are tasked with enhancing the resolution of a video. The goal is to improve the quality of individual frames. You are expected to use basic algorithms for achieving this goal. \n",
    "\n",
    "### Task 1: Frame Extraction\n",
    "\n",
    "Extract frames from the video using OpenCV.\n",
    "\n",
    "### Task 2: Resolution Enhancement\n",
    "\n",
    "Apply the following enhancement algorithms to scale the extracted frames by a factor of 2:\n",
    "\n",
    "1) Nearest-neighbor Interpolation <br>\n",
    "2) Bilinear Interpolation <br>\n",
    "3) Bicubic Interpolation <br>\n",
    "\n",
    "Explore these approaches by your self. These are just builtin parameters in resize function.\n",
    "https://theailearner.com/2018/11/15/image-interpolation-using-opencv-python/\n",
    "\n",
    "### Task 3: Video Reconstruction\n",
    "\n",
    "After enhancing the frames, reconstruct the video by merging the enhanced frames while ensuring that the frame rate of the reconstructed video matches that of the original video. Generate a separate video for each interpolation method.\n",
    "\n",
    "<b>Bonus</b>: Apply a self-selected algorithm to improve video quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a7eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vidcap = cv2.VideoCapture('Q1.mp4')\n",
    "# yeh function video say image ju read karay ga\n",
    "# now we will extract frames using while loop\n",
    "count = 0\n",
    "while vidcap.isOpened():\n",
    "    # ju frames open ha wu pas karay ga\n",
    "    succ, Image = vidcap.read()\n",
    "    # yaha image save hu gi\n",
    "    cv2.imwrite('Image%d.png' % count,Image )\n",
    "    count+=1\n",
    "    # imwrite destination path mangta ha k kidhar save karani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f21df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_files = []\n",
    "for i in range(239):\n",
    "    frame_files.append('Image%d.png' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the quality of Initial frames by using Neighbouring Interpolation\n",
    "# reading all initial files\n",
    "# initial images stored in frame_files[]\n",
    "# initializing scale factor of 2\n",
    "image_path = '/Users/urwah/Desktop/Assign5/'\n",
    "sf = 2\n",
    "count = 0\n",
    "# image = cv2.imread('Image0.png')\n",
    "# nwidth = image.shape[1]*sf\n",
    "# nheight = image.shape[0]*sf\n",
    "# for new scale factor we have to calculate new width and height\n",
    "for i in range(240):\n",
    "    image = cv2.imread('Image%d.png'%count)\n",
    "    nwidth = image.shape[1]*sf\n",
    "    nheight = image.shape[0]*sf\n",
    "    # once the dimensions are finalised we will resize each image\n",
    "    # using neighbouring interpolation\n",
    "    new_img = cv2.resize(image,(nwidth,nheight),interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite('Output_image%d.png' % count,new_img)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating output video from these frames\n",
    "Neighbour_interpolation_frames = []\n",
    "for i in range(240):\n",
    "    Neighbour_interpolation_frames.append('Output_image%d.png' % i)\n",
    "# Neighbour_interpolation_frames[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1 accurate of neighbourig interpolation\n",
    "import cv2\n",
    "# path to read the frames\n",
    "image_path = '/Users/urwah/Desktop/Assign5/'  \n",
    "output_video_file = 'Neighbour_Interpolation.mp4'\n",
    "\n",
    "# frame rate and size based on the the very first frame in the video\n",
    "first_image = cv2.imread(image_path + Neighbour_interpolation_frames[0])\n",
    "frame_height, frame_width, _ = first_image.shape\n",
    "frame_rate = 30  \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "out = cv2.VideoWriter(output_video_file, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "# Adding images to the environment created for video\n",
    "for image_file in Neighbour_interpolation_frames:\n",
    "    frame = cv2.imread(image_path + image_file)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2 accurate of bilinear interpolation frames\n",
    "# changing the quality of Initial frames by using Neighbouring Interpolation\n",
    "# reading all initial files\n",
    "# initial images stored in frame_files[]\n",
    "# initializing scale factor of 2\n",
    "image_path = '/Users/urwah/Desktop/Assign5/'\n",
    "sf = 2\n",
    "count = 0\n",
    "# image = cv2.imread('Image0.png')\n",
    "# nwidth = image.shape[1]*sf\n",
    "# nheight = image.shape[0]*sf\n",
    "# for new scale factor we have to calculate new width and height\n",
    "for i in range(240):\n",
    "    image = cv2.imread('Image%d.png'%count)\n",
    "    nwidth = image.shape[1]*sf\n",
    "    nheight = image.shape[0]*sf\n",
    "    # once the dimensions are finalised we will resize each image\n",
    "    # using neighbouring interpolation\n",
    "    new_img = cv2.resize(image,(nwidth,nheight),interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.imwrite('Linear%d.png' % count,new_img)\n",
    "    count+=1\n",
    "\n",
    "linear_interpolation_frames = []\n",
    "for i in range(240):\n",
    "    linear_interpolation_frames.append('Linear%d.png' % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Bi-Linear Video\n",
    "import cv2\n",
    "# path to read the frames\n",
    "image_path = '/Users/urwah/Desktop/Assign5/'  \n",
    "output_video_file = 'BiLinear_Interpolation.mp4'\n",
    "\n",
    "# frame rate and size based on the the very first frame in the video\n",
    "first_image = cv2.imread(image_path + linear_interpolation_frames[0])\n",
    "frame_height, frame_width, _ = first_image.shape\n",
    "frame_rate = 29.97002997002997 \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "out = cv2.VideoWriter(output_video_file, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "# Adding images to the environment created for video\n",
    "for image_file in linear_interpolation_frames:\n",
    "    frame = cv2.imread(image_path + image_file)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fda9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I am enhancing the video quality of Bi-Linear Interpolation Video\n",
    "import cv2\n",
    "video_capture = cv2.VideoCapture('BiLinear_Interpolation.mp4')  \n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open the Video.\")\n",
    "else:\n",
    "    # Frames per second (FPS) of the video\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    # Frame Dims\n",
    "    new_frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    new_frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter('Enhanced_BL.mp4',fourcc, fps, (new_frame_width, new_frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (new_frame_width, new_frame_height))\n",
    "        # Here I am sharpening the image so that it gets less blurry using laplacian kernel\n",
    "        # -1 to indicate that the output image will have the same depth as the input image.\n",
    "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32)\n",
    "        # filter2D\n",
    "        sharpened_frame = cv2.filter2D(frame, -1, kernel)\n",
    "        output_video.write(sharpened_frame)\n",
    "\n",
    "    # Release the videos\n",
    "    video_capture.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b31f0e",
   "metadata": {},
   "source": [
    "**BICUBIC INTERPOLATION VIDEO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the frames \n",
    "image_path = '/Users/urwah/Desktop/Assign5/'\n",
    "sf = 2\n",
    "count = 0\n",
    "# for new scale factor we have to calculate new width and height\n",
    "for i in range(240):\n",
    "    image = cv2.imread('Image%d.png'%count)\n",
    "    nwidth = image.shape[1]*sf\n",
    "    nheight = image.shape[0]*sf\n",
    "    # once the dimensions are finalised we will resize each image\n",
    "    # using neighbouring interpolation\n",
    "    new_img = cv2.resize(image,(nwidth,nheight),interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imwrite('bicubic%d.png' % count,new_img)\n",
    "    count+=1\n",
    "    \n",
    "bicubic_interpolation_frames = []\n",
    "for i in range(240):\n",
    "    bicubic_interpolation_frames.append('Output_image%d.png' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Bi-Cubic Video\n",
    "import cv2\n",
    "# path to read the frames\n",
    "image_path = '/Users/urwah/Desktop/Assign5/'  \n",
    "output_video_file = 'Bicubic_Interpolation.mp4'\n",
    "\n",
    "# frame rate and size based on the the very first frame in the video\n",
    "first_image = cv2.imread(image_path + bicubic_interpolation_frames[0])\n",
    "frame_height, frame_width, _ = first_image.shape\n",
    "frame_rate = 29.97002997002997 \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "out = cv2.VideoWriter(output_video_file, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "# Adding images to the environment created for video\n",
    "for image_file in bicubic_interpolation_frames:\n",
    "    frame = cv2.imread(image_path + image_file)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acad5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Applying kernel to the video so that it gets enhances\n",
    "import cv2\n",
    "video_capture = cv2.VideoCapture('Bicubic_Interpolation.mp4')  \n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open the Video.\")\n",
    "else:\n",
    "    # Frames per second (FPS) of the video\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    # Frame Dims\n",
    "    new_frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    new_frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter('Enhanced_BCubic.mp4',fourcc, fps, (new_frame_width, new_frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (new_frame_width, new_frame_height))\n",
    "        # Here I am sharpening the image so that it gets less blurry using laplacian kernel\n",
    "        # -1 to indicate that the output image will have the same depth as the input image.\n",
    "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32)\n",
    "        # filter2D\n",
    "        sharpened_frame = cv2.filter2D(frame, -1, kernel)\n",
    "        output_video.write(sharpened_frame)\n",
    "\n",
    "    # Release the videos\n",
    "    video_capture.release()\n",
    "    output_video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83645d46",
   "metadata": {},
   "source": [
    "# Question: 2\n",
    "\n",
    "In this question, you are tasked with enhancing the audio quality of the video. Follow the given procedure to increase audio quality.\n",
    "\n",
    "### Step 1: Short-Time Fourier Transform (STFT)\n",
    "Compute the Short-Time Fourier Transform (STFT) of the audio signal. This operation transforms the audio into the frequency domain over short time intervals.\n",
    "\n",
    "### Step 2: Magnitude and Phase Extraction\n",
    "From the STFT, get the magnitude and phase using the np.abs() and np.angle() functions.\n",
    "\n",
    "### Step 3: Noise Profile Creation\n",
    "Load the noisy audio and calculate its STFT and magnitude from the STFT. Afterward, compute the average magnitude of the audio along axis=1 to generate a noise profile. This profile is essential for identifying and removing noise.\n",
    "\n",
    "### Step 4: Adjusting with a Hyperparameter\n",
    "Multiply the noise profile array by a hyperparameter represented as alpha. Experiment with various values of alpha to fine-tune the results. A good starting point is to set alpha to 2.\n",
    "\n",
    "### Step 5: Audio Denoising\n",
    "Subtract the mean noise array from the original audio (You may need to adjust the dimensions of the mean noise array to match with original audio). Ensure that any negative values in the resulting array are replaced with 0. This step effectively reduces noise in the audio.\n",
    "\n",
    "### Step 6: Incorporating Phase Information\n",
    "Multiply the modified audio by the complex exponential of the phase information obtained in step 3, which can be expressed as np.exp(1.0j * phase).\n",
    "\n",
    "### Step 7: Inverse Short-Time Fourier Transform (ISTFT)\n",
    "Reconstruct the audio by performing the Inverse Short Time Fourier Transform (ISTFT) on the modified audio signal using librosa. Save the resulting audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values_wav, sr = librosa.load(Q2.wav)           \n",
    "print(values_wav.shape)\n",
    "\n",
    "# Step 1\n",
    "Stftt = librosa.stft(values_wav)\n",
    "print(Stftt.shape)\n",
    "\n",
    "# Step 2\n",
    "mag_orig = np.abs(Stftt)                    # magnitude of Stftt\n",
    "phase = np.angle(Stftt)                     # phase of Stftt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664aba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3\n",
    "values_wav_noise, sr = librosa.load(Q2-Noise.wav)             # values_wav_noise is a numpy array of the wav file, sr is the sampling rate\n",
    "print(values_wav_noise.shape)\n",
    "\n",
    "N_stft = librosa.stft(values_wav_noise)\n",
    "mag_noisy = np.abs(N_stft)                          # magnitude\n",
    "\n",
    "noice_profile = np.mean(mag_noisy, axis=1)              # mean\n",
    "\n",
    "phase_noice_wav = np.angle(N_stft)                  # phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4\n",
    "alpha = 3       # alpha's value 4 is also good\n",
    "noice_profile = alpha*(noice_profile)\n",
    "\n",
    "\n",
    "# Step 5 (Denoising)\n",
    "noice_profile = np.reshape(noice_profile, (noice_profile.shape[0], 1))               # adding a new axis to noice_profile\n",
    "\n",
    "arr_denoised = mag_orig - noice_profile\n",
    "\n",
    "arr_denoised[arr_denoised  0] = 0                  # replacing negative values with 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c14746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6\n",
    "arr_denoised = arr_denoised[, 560]  np.exp(1.0j  phase_noice_wav)[, 560]         # multiplying with phase_noice_wav\n",
    "\n",
    "# Step 7\n",
    "arr_denoised = librosa.istft(arr_denoised)                          # ISTFT of arr_denoised\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "sf.write('Q2_Final.wav', arr_denoised, sr)              # saving the denoised audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc88728",
   "metadata": {},
   "source": [
    "# Question: 3\n",
    "\n",
    "For this task, use whisper inference to generate text from the audio file. Use any translation library to translate the text into another language, and then utilize a TTS system to produce audio from the translated text. Supported Languages are :English, Urdu, Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602468b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for this task use whisper inference to generate text from my audio file use any translation library to translate the text into another language and then utilise a TTS system to produce ODI from the translated text\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile('Q3.wav') as source:\n",
    "    audio_text = r.listen(source)\n",
    "text = r.recognize_google(audio_text)\n",
    "print(text)\n",
    "audio_text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1096fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لهذه المهمة استخدم استدلال الهمس لإنشاء نص من ملفي الصوتي استخدم أي مكتبة ترجمة لترجمة النص إلى لغة أخرى ثم استخدم نظام TTS لإنتاج ODI من النص المترجم\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator = Translator(to_lang=\"ar\")\n",
    "translated_text = translator.translate(audio_text)\n",
    "print(f\"{translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00df6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "arabic_text = translated_text\n",
    "tts = gTTS(text=arabic_text, lang='ar')\n",
    "tts.save('arabic_audio.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48160865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس کام کے لئے میری آڈیو فائل سے متن پیدا کرنے کے لئے سرگوشی کا استعمال کریں متن کو کسی دوسری زبان میں ترجمہ کرنے کے لئے کسی بھی ترجمے کی لائبریری کا استعمال کریں اور پھر ترجمہ شدہ متن سے ون ڈے تیار کرنے کے لئے ٹی ٹی ایس سسٹم کا استعمال کریں\n"
     ]
    }
   ],
   "source": [
    "# translating in urdu\n",
    "from translate import Translator\n",
    "translator = Translator(to_lang=\"ur\")\n",
    "translated_text = translator.translate(audio_text)\n",
    "print(f\"{translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae3db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generating audio\n",
    "from gtts import gTTS\n",
    "urdu_text = translated_text\n",
    "tts = gTTS(text=urdu_text, lang='ur')\n",
    "tts.save('urdu_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac170a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
